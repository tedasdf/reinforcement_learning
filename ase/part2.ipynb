{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25e35b4",
   "metadata": {},
   "source": [
    "Dynamic programming \n",
    "\n",
    "\n",
    "refeers to algorithm used to find optimal policies which have complete knowledge of the environement as an MDP\n",
    "\n",
    "\n",
    "meaning : I have \n",
    "\n",
    "\n",
    "the p(s' , r | s,a)\n",
    "\n",
    "\n",
    "the probability of next state and reward given the previous s and action :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b7a12",
   "metadata": {},
   "source": [
    "The BELLMAN EQuation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c00b9",
   "metadata": {},
   "source": [
    "v_pi(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0e0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. There is an agent selecting the next state from init state\n",
    "\n",
    "\n",
    "init_state = 0 \n",
    "states = [-2, -1, 0 , 1, 2]\n",
    "action = ['left', 'right']\n",
    "\n",
    "\n",
    "probability_state = { \n",
    "    'right' : {1: [0.12 , 0.22 , 0.20], 2: [0.09, 0.32, 0.05]},\n",
    "    'left' : {-2: [0.34 , 0.05, 0.17], -1: [0.17, 0.23 , 0.04]}\n",
    "}\n",
    "# probabilioty goes like this action as key then states as key then index as reward \n",
    "# this is basically p(s', r | s, a)\n",
    "\n",
    "probability_action = {'left': 0.4 , 'right': 0.6}\n",
    "\n",
    "discount_rate = 0.95\n",
    "\n",
    "\n",
    "states_value = {-2:19.2 , -1:16.5 , 1:18.1, 2:16.2}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177fd0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s | take RIGHT) = 17.4047\n",
      "V(s | take LEFT)  = 17.8114\n"
     ]
    }
   ],
   "source": [
    "def bellman_update(prob_dict, states_value, gamma):\n",
    "    \"\"\"prob_dict = probability_state['right'] or ['left']\"\"\"\n",
    "    v = 0.0\n",
    "    for s_prime, reward_probs in prob_dict.items():\n",
    "        for r, p in enumerate(reward_probs):\n",
    "            v += p * (r + gamma * states_value[s_prime])\n",
    "    return v\n",
    "\n",
    "\n",
    "# compute the value given each action\n",
    "V_right = bellman_update(probability_state['right'], states_value, discount_rate)\n",
    "V_left  = bellman_update(probability_state['left'],  states_value, discount_rate)\n",
    "\n",
    "print(\"V(s | take RIGHT) =\", V_right)\n",
    "print(\"V(s | take LEFT)  =\", V_left)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8d6af",
   "metadata": {},
   "source": [
    "Bellman optimality\n",
    "\n",
    "\n",
    "v*(s) = max v_pi(s)\n",
    "\n",
    "\n",
    "q*(s,a) = max q_pi(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.2\n",
      "0.1\n",
      "-0.2\n",
      "0.3\n",
      "-0.1\n",
      "0.8\n",
      "0.9\n",
      "-0.2\n",
      "0.6\n",
      "0.1\n",
      "0.1\n",
      "0.9\n",
      "-0.9\n"
     ]
    }
   ],
   "source": [
    "# Policy Evaluation and Polcy Improvement \n",
    "\n",
    "\n",
    "list_state_val = [\n",
    "    [0.0 , 0.4, 0.2, 0.1],\n",
    "    [-0.2 , 0.3, -0.1, 0.8],\n",
    "    [0.9, -0.2, 0.6, 0.1],\n",
    "    [0.1, 0.9, -0.9 , 0.0]\n",
    "]\n",
    "\n",
    "Rt = -1\n",
    "pi_a_s = 0.25 # four direction equal probability\n",
    "\n",
    "\n",
    "# it doesnt satisfy bellman equation not valid state values\n",
    "action = [\n",
    "    [-1, 0],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, -1],\n",
    "] \n",
    "\n",
    "for i in range(len(list_state_val)):\n",
    "    for j in range(len(list_state_val[i])):\n",
    "        if list_state_val[i][j] == 0:\n",
    "            continue\n",
    "        print(list_state_val[i][j])\n",
    "        sum = 0 \n",
    "        for act in action:\n",
    "            i, j = i-act[0] , j -act[1]\n",
    "            # clip them \n",
    "\n",
    "            sum += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fec26c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (given):\n",
      " [[ 0.   0.4  0.2  0.1]\n",
      " [-0.2  0.3 -0.1  0.8]\n",
      " [ 0.9 -0.2  0.6  0.1]\n",
      " [ 0.1  0.9 -0.9  0. ]]\n",
      "\n",
      "Bellman RHS under deterministic model:\n",
      " [[ 0.     -0.7975 -0.865  -0.73  ]\n",
      " [-0.775  -1.0225 -0.5725 -0.7975]\n",
      " [-0.865  -0.3925 -1.2475 -0.6625]\n",
      " [-0.55   -1.0225 -0.865   0.    ]]\n",
      "\n",
      "Residual = RHS - V:\n",
      " [[ 0.     -1.1975 -1.065  -0.83  ]\n",
      " [-0.575  -1.3225 -0.4725 -1.5975]\n",
      " [-1.765  -0.1925 -1.8475 -0.7625]\n",
      " [-0.65   -1.9225  0.035   0.    ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# given V (4x4)\n",
    "list_state_val = [\n",
    "    [0.0 , 0.4, 0.2, 0.1],\n",
    "    [-0.2 , 0.3, -0.1, 0.8],\n",
    "    [0.9, -0.2, 0.6, 0.1],\n",
    "    [0.1, 0.9, -0.9 , 0.0]\n",
    "]\n",
    "V = np.array(list_state_val, dtype=float)      # shape (4,4)\n",
    "\n",
    "rows, cols = V.shape\n",
    "Rt = -1\n",
    "pi_a_s = 0.25  # uniform over 4 actions\n",
    "gamma = 0.9    # discount factor\n",
    "\n",
    "# actions: up, down, right, left\n",
    "actions = [(-1,0),(1,0),(0,1),(0,-1)]\n",
    "\n",
    "# terminal states (top-left and bottom-right)\n",
    "terminal_states = {(0,0), (rows-1, cols-1)}\n",
    "\n",
    "# ensure terminal V is zero\n",
    "for (ti, tj) in terminal_states:\n",
    "    V[ti, tj] = 0.0\n",
    "\n",
    "def clip(i,j):\n",
    "    return min(max(0,i), rows-1), min(max(0,j), cols-1)\n",
    "\n",
    "# Build deterministic p(s',r|s,a)\n",
    "# For terminal states: treat as absorbing with reward 0 (self-loop)\n",
    "p = {}\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        p[(i,j)] = {}\n",
    "        for a_idx, (di,dj) in enumerate(actions):\n",
    "            if (i,j) in terminal_states:\n",
    "                # absorbing: stays in terminal and gives 0 reward\n",
    "                p[(i,j)][a_idx] = { (i,j): { 0: 1.0 } }\n",
    "            else:\n",
    "                ni, nj = clip(i+di, j+dj)\n",
    "                # deterministic next-state with reward Rt (non-terminal)\n",
    "                p[(i,j)][a_idx] = { (ni, nj): { Rt: 1.0 } }\n",
    "\n",
    "# Compute Bellman RHS for each state under the uniform policy\n",
    "RHS = np.zeros_like(V, dtype=float)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        # If terminal, we keep RHS = V (or compute via self-loop, same effect)\n",
    "        total = 0.0\n",
    "        for a_idx in range(len(actions)):\n",
    "            trans = p[(i,j)][a_idx]\n",
    "            for (ni, nj), reward_dict in trans.items():\n",
    "                for r, prob in reward_dict.items():\n",
    "                    total += pi_a_s * prob * (r + gamma * V[ni, nj])\n",
    "        RHS[i, j] = total\n",
    "\n",
    "# Residual = RHS - V : should be 0 for states that already satisfy Bellman\n",
    "residual = RHS - V\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(\"V (given):\\n\", V)\n",
    "print(\"\\nBellman RHS under deterministic model:\\n\", RHS)\n",
    "print(\"\\nResidual = RHS - V:\\n\", residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa9f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ae89a4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
