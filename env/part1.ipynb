{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_state = {0 , 1, 2}\n",
    "\n",
    "action = {'up' , 'down'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99d02b",
   "metadata": {},
   "source": [
    "Policy = p(s', r| s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e77e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis = [0,1,2,3,4,5]\n",
    "\n",
    "\n",
    "lis[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0af5ac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0, Action -1:\n",
      "  Next state 0: {0: np.float64(0.09941869985442059), 1: np.float64(0.2529879017207762), -1: np.float64(0.6475933984248032)}\n",
      "  Next state 1: {0: np.float64(0.1001195157028509), 1: np.float64(0.6789864456838027), -1: np.float64(0.22089403861334642)}\n",
      "  Next state 2: {0: np.float64(0.5055531361615325), 1: np.float64(0.022229364127940337), -1: np.float64(0.4722174997105271)}\n",
      "State 0, Action 1:\n",
      "  Next state 0: {0: np.float64(0.3294001724996741), 1: np.float64(0.4340973720321602), -1: np.float64(0.23650245546816565)}\n",
      "  Next state 1: {0: np.float64(0.1180742640794125), 1: np.float64(0.8414599449341414), -1: np.float64(0.04046579098644603)}\n",
      "  Next state 2: {0: np.float64(0.3753695115435407), 1: np.float64(0.10008965519921276), -1: np.float64(0.5245408332572467)}\n",
      "State 1, Action -1:\n",
      "  Next state 0: {0: np.float64(0.2664096103847175), 1: np.float64(0.12409868215917462), -1: np.float64(0.6094917074561078)}\n",
      "  Next state 1: {0: np.float64(0.1358001987395143), 1: np.float64(0.8271978617136546), -1: np.float64(0.037001939546830974)}\n",
      "  Next state 2: {0: np.float64(0.27483900761467417), 1: np.float64(0.28270840773805916), -1: np.float64(0.44245258464726667)}\n",
      "State 1, Action 1:\n",
      "  Next state 0: {0: np.float64(0.028477829107710257), 1: np.float64(0.7987672326589061), -1: np.float64(0.17275493823338364)}\n",
      "  Next state 1: {0: np.float64(0.14499778728394672), 1: np.float64(0.6440375053270038), -1: np.float64(0.21096470738904935)}\n",
      "  Next state 2: {0: np.float64(0.2646984064548501), 1: np.float64(0.3610696617201462), -1: np.float64(0.3742319318250037)}\n",
      "State 2, Action -1:\n",
      "  Next state 0: {0: np.float64(0.22801543358419998), 1: np.float64(0.3086850609560329), -1: np.float64(0.4632995054597671)}\n",
      "  Next state 1: {0: np.float64(0.29527241155256556), 1: np.float64(0.5230191692209517), -1: np.float64(0.1817084192264827)}\n",
      "  Next state 2: {0: np.float64(0.4130825158831029), 1: np.float64(0.22441359701854124), -1: np.float64(0.36250388709835585)}\n",
      "State 2, Action 1:\n",
      "  Next state 0: {0: np.float64(0.11843161778060664), 1: np.float64(0.5104696118413943), -1: np.float64(0.3710987703779991)}\n",
      "  Next state 1: {0: np.float64(0.37674991502137295), 1: np.float64(0.3219986545860461), -1: np.float64(0.30125143039258095)}\n",
      "  Next state 2: {0: np.float64(0.13649672118366907), 1: np.float64(0.38584203277352047), -1: np.float64(0.4776612460428105)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "states = [0, 1, 2]\n",
    "actions = [-1, 1]\n",
    "rewards = [0, 1, -1]\n",
    "\n",
    "# Initialize dictionary\n",
    "reward_dist = {}\n",
    "\n",
    "for s in states:\n",
    "    reward_dist[s] = {}\n",
    "    for a in actions:\n",
    "        reward_dist[s][a] = {}\n",
    "        # Random probabilities for next states\n",
    "        next_state_probs = np.random.rand(len(states))\n",
    "        next_state_probs /= next_state_probs.sum()\n",
    "        \n",
    "        for i, next_s in enumerate(states):\n",
    "            reward_dist[s][a][next_s] = {}\n",
    "            # Random probabilities for rewards given next_state\n",
    "            reward_probs = np.random.rand(len(rewards))\n",
    "            reward_probs /= reward_probs.sum()\n",
    "            for j, r in enumerate(rewards):\n",
    "                reward_dist[s][a][next_s][r] = reward_probs[j]\n",
    "\n",
    "# Print example\n",
    "for s in reward_dist:\n",
    "    for a in reward_dist[s]:\n",
    "        print(f\"State {s}, Action {a}:\")\n",
    "        for next_s in reward_dist[s][a]:\n",
    "            print(f\"  Next state {next_s}: {reward_dist[s][a][next_s]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51857348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: 1\n",
      "Selected action: 0\n"
     ]
    }
   ],
   "source": [
    "act_give_state = [1/2] * 2\n",
    "starting_probs = [1/3] * 3\n",
    "\n",
    "action_type = {-1:'up', 1:'down'}\n",
    "\n",
    "states_type = ['#B2B2FF', '#6666FF', '#1919FF']\n",
    "\n",
    "\n",
    "init_state = np.random.choice(states, p=starting_probs)\n",
    "print(f\"Starting state index: {init_state}\")\n",
    "HTML(f'<div style=\"width:50px;height:50px;background-color:{states_type[init_state]}\"></div>')\n",
    "\n",
    "\n",
    "\n",
    "policy_pi = {s: act_give_state for s in states}\n",
    "\n",
    "actions_probs = policy_pi[init_state]\n",
    "action = np.random.choice(actions, p=actions_probs)\n",
    "print(f\"Selected action: {action}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19a40d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state index: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width:50px;height:50px;background-color:#1919FF\"></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken: down\n",
      "NEXT STATE: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "# Probabilities\n",
    "act_give_state = [1/2] * 2        # probability for each action (uniform)\n",
    "starting_probs = [1/3] * 3        # probability for each starting state (uniform)\n",
    "\n",
    "# Map reward to action name (optional)\n",
    "action_type = {-1:'down', 1:'down'}\n",
    "\n",
    "# Colors for each state (increasing saturation of blue)\n",
    "states_type = ['#B2B2FF', '#6666FF', '#1919FF']\n",
    "\n",
    "# Sample initial state according to starting distribution\n",
    "states = [0, 1, 2]\n",
    "init_state = np.random.choice(states, p=starting_probs)\n",
    "\n",
    "print(f\"Starting state index: {init_state}\")\n",
    "display(HTML(f'<div style=\"width:50px;height:50px;background-color:{states_type[init_state]}\"></div>'))\n",
    "\n",
    "policy_pi = {s: act_give_state for s in states}\n",
    "\n",
    "# 2. Sample an action according to policy\n",
    "action_probs = policy_pi[init_state]\n",
    "action = np.random.choice(actions, p=action_probs)\n",
    "print(f\"Action taken: {action_type[action]}\")\n",
    "\n",
    "\n",
    "next_s = init_state + action\n",
    "\n",
    "# Clip to valid state indices (0, 1, 2)\n",
    "next_s = min(max(0, next_s), 2)\n",
    "\n",
    "print(f\"NEXT STATE: {next_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1b7c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_val_func(G_t , S_t):\n",
    "    return \n",
    "\n",
    "def act_val_func(G_t, S_t , A_t):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95caed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find an optimal policy pi*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
